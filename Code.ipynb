{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JJ1G10L4cx-z",
        "DNziQXhWdPPc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Common parts for end point prediction and trajectory completion**"
      ],
      "metadata": {
        "id": "uHElbb2Ub1-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Connect to google drive and download dataset**"
      ],
      "metadata": {
        "id": "bFbVPKetcBQi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrY0kQ61bgRq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "!mkdir waymo_dataset\n",
        "!gcsfuse --implicit-dirs waymo_open_dataset_motion_v_1_0_0 waymo_dataset\n",
        "!pip3 install --upgrade pip\n",
        "!pip3 install waymo-open-dataset-tf-2-6-0\n",
        "!pip install keras==2.6.0"
      ],
      "metadata": {
        "id": "s3loXjz3cHcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Necessary packages**"
      ],
      "metadata": {
        "id": "EMOzhcVVcPEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import uuid\n",
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, Reshape, BatchNormalization\n",
        "from keras.layers import Conv1D, Dropout, Activation, MaxPool1D\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.initializers import GlorotUniform"
      ],
      "metadata": {
        "id": "_P_OA3ijcWIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Global variables**"
      ],
      "metadata": {
        "id": "E9j3fcHBcmDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_USED_FILES = 1000\n",
        "NUM_USED_FILES_VALID = 75\n",
        "\n",
        "# Hyperparameters\n",
        "BS = 128  #Batch size\n",
        "BSV = 128 #Batch size for validation set\n",
        "\n",
        "EPOCHS = 15\n",
        "START_LR = 0.01\n",
        "START_LR_TRAJ = 0.01\n",
        "DROPOUT_RATE_SPATIAL = 0.4\n",
        "\n",
        "# The same file to be used as visualization of the prediction as a proof\n",
        "FILENAME = '/content/waymo_dataset/uncompressed/tf_example/training/training_tfexample.tfrecord-00000-of-01000'    \n",
        "VALIDATION_FILENAME = '/content/waymo_dataset/uncompressed/tf_example/validation/validation_tfexample.tfrecord-00000-of-00150'    \n",
        "\n",
        "STEP_TO_VISUALIZE = 0\n",
        "EPOCH_TO_VISUALIZE = 2 # Chosen randomly\n",
        "\n",
        "LATENT_DIM = 256  # The dimensions of the latent variables (and c in the LSTM model)"
      ],
      "metadata": {
        "id": "E2uXGMz4ckvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get dataset**"
      ],
      "metadata": {
        "id": "JJ1G10L4cx-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roadgraph_features = {\n",
        "    \"roadgraph_samples/dir\": tf.io.FixedLenFeature(\n",
        "        [20000, 3], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"roadgraph_samples/id\": tf.io.FixedLenFeature(\n",
        "        [20000, 1], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"roadgraph_samples/type\": tf.io.FixedLenFeature(\n",
        "        [20000, 1], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"roadgraph_samples/valid\": tf.io.FixedLenFeature(\n",
        "        [20000, 1], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"roadgraph_samples/xyz\": tf.io.FixedLenFeature(\n",
        "        [20000, 3], tf.float32, default_value=None\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Features of other agents.\n",
        "state_features = {\n",
        "    \"state/id\": tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
        "    \"state/type\": tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
        "    \"state/is_sdc\": tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
        "    \"state/tracks_to_predict\": tf.io.FixedLenFeature(\n",
        "        [128], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"state/current/bbox_yaw\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/current/height\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/current/length\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/current/timestamp_micros\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"state/current/valid\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"state/current/vel_yaw\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/current/velocity_x\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/current/velocity_y\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/current/speed\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/current/width\": tf.io.FixedLenFeature(\n",
        "        [128, 1], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/current/x\": tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    \"state/current/y\": tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    \"state/current/z\": tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
        "    \"state/future/bbox_yaw\": tf.io.FixedLenFeature(\n",
        "        [128, 80], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/future/height\": tf.io.FixedLenFeature(\n",
        "        [128, 80], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/future/length\": tf.io.FixedLenFeature(\n",
        "        [128, 80], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/future/timestamp_micros\": tf.io.FixedLenFeature(\n",
        "        [128, 80], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"state/future/valid\": tf.io.FixedLenFeature(\n",
        "        [128, 80], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"state/future/vel_yaw\": tf.io.FixedLenFeature(\n",
        "        [128, 80], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/future/velocity_x\": tf.io.FixedLenFeature(\n",
        "        [128, 80], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/future/velocity_y\": tf.io.FixedLenFeature(\n",
        "        [128, 80], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/future/width\": tf.io.FixedLenFeature(\n",
        "        [128, 80], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/future/x\": tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    \"state/future/y\": tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    \"state/future/z\": tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
        "    \"state/past/bbox_yaw\": tf.io.FixedLenFeature(\n",
        "        [128, 10], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/past/height\": tf.io.FixedLenFeature(\n",
        "        [128, 10], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/past/length\": tf.io.FixedLenFeature(\n",
        "        [128, 10], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/past/timestamp_micros\": tf.io.FixedLenFeature(\n",
        "        [128, 10], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"state/past/valid\": tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
        "    \"state/past/vel_yaw\": tf.io.FixedLenFeature(\n",
        "        [128, 10], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/past/velocity_x\": tf.io.FixedLenFeature(\n",
        "        [128, 10], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/past/velocity_y\": tf.io.FixedLenFeature(\n",
        "        [128, 10], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/past/speed\": tf.io.FixedLenFeature(\n",
        "        [128, 10], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/past/width\": tf.io.FixedLenFeature(\n",
        "        [128, 10], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"state/past/x\": tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    \"state/past/y\": tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    \"state/past/z\": tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
        "    \"scenario/id\": tf.io.FixedLenFeature([1], tf.string, default_value=None),\n",
        "}\n",
        "\n",
        "traffic_light_features = {\n",
        "    \"traffic_light_state/current/state\": tf.io.FixedLenFeature(\n",
        "        [1, 16], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"traffic_light_state/current/valid\": tf.io.FixedLenFeature(\n",
        "        [1, 16], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"traffic_light_state/current/id\": tf.io.FixedLenFeature(\n",
        "        [1, 16], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"traffic_light_state/current/x\": tf.io.FixedLenFeature(\n",
        "        [1, 16], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"traffic_light_state/current/y\": tf.io.FixedLenFeature(\n",
        "        [1, 16], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"traffic_light_state/current/z\": tf.io.FixedLenFeature(\n",
        "        [1, 16], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"traffic_light_state/past/state\": tf.io.FixedLenFeature(\n",
        "        [10, 16], tf.int64, default_value=None\n",
        "    ),\n",
        "    \"traffic_light_state/past/valid\": tf.io.FixedLenFeature(\n",
        "        [10, 16], tf.int64, default_value=None\n",
        "    ),\n",
        "    # \"traffic_light_state/past/id\":\n",
        "    # tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
        "    \"traffic_light_state/past/x\": tf.io.FixedLenFeature(\n",
        "        [10, 16], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"traffic_light_state/past/y\": tf.io.FixedLenFeature(\n",
        "        [10, 16], tf.float32, default_value=None\n",
        "    ),\n",
        "    \"traffic_light_state/past/z\": tf.io.FixedLenFeature(\n",
        "        [10, 16], tf.float32, default_value=None\n",
        "    ),\n",
        "}\n",
        "\n",
        "features_description = {}\n",
        "features_description.update(roadgraph_features)\n",
        "features_description.update(state_features)\n",
        "features_description.update(traffic_light_features)\n"
      ],
      "metadata": {
        "id": "TrFZufT7cwvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _parse_model_ready(value):\n",
        "  \"\"\"\n",
        "  This function parses 1 scenario (where we max can have 128 different objects) out of many in 1 .tf file \n",
        "  into its basic elements. It returns all the necessary information about the scenario such as velocity \n",
        "  and position of each object for all the time points, which of the objects do currently or have/will exist\n",
        "  decoded_example = tf.io.parse_single_example(value, features_description)\n",
        "\n",
        "  Inputs:\n",
        "    value: 1 tf file that is going to be parsed\n",
        "  \n",
        "  Returns:\n",
        "    inputs: a dictionary that contains all the necessary information to do prediction about the future. \n",
        "  \"\"\"\n",
        "  decoded_example = tf.io.parse_single_example(value, features_description)\n",
        "\n",
        "\n",
        "  #past_states includes all relevant information about pasts states that does not need to be normalized\n",
        "  past_states = tf.stack([\n",
        "      #Only includes valid values for each of the features by masking with valid vector. Currently sets all invalid enties to zero, but can easily be changed\n",
        "      tf.where(decoded_example['state/past/valid'] > 0, decoded_example['state/past/speed'], tf.zeros_like(decoded_example['state/past/speed'])),\n",
        "      tf.where(decoded_example['state/past/valid'] > 0, decoded_example['state/past/bbox_yaw'], tf.zeros_like(decoded_example['state/past/bbox_yaw'])),\n",
        "      tf.where(decoded_example['state/past/valid'] > 0, decoded_example['state/past/velocity_x'], tf.zeros_like(decoded_example['state/past/velocity_x'])),\n",
        "      tf.where(decoded_example['state/past/valid'] > 0, decoded_example['state/past/velocity_y'], tf.zeros_like(decoded_example['state/past/velocity_y'])),\n",
        "      tf.where(decoded_example['state/past/valid'] > 0, decoded_example['state/past/vel_yaw'], tf.zeros_like(decoded_example['state/past/vel_yaw']))\n",
        "  ], 1)  # shape (128,5,10) --> 128 = #of objects in the scenario, 5 properties (speed,bbox_yaw, ... vel_yaw), 10 steps in the past\n",
        "  \n",
        "  #past_xyz includes all relevant information about past states that needs to be normalized\n",
        "  past_xyz = tf.stack([\n",
        "      decoded_example['state/past/x'], \n",
        "      decoded_example['state/past/y'],\n",
        "      decoded_example['state/past/z'],\n",
        "  ], 1) # shape (128,3,10)\n",
        "\n",
        "\n",
        "  #Same as for pasts_states but for current time point, see comment above\n",
        "  cur_states = tf.stack([\n",
        "      #Only includes valid values for each of the features by masking with valid vector. Currently sets all invalid entries to zero, but can easily be changed\n",
        "      tf.where(decoded_example['state/current/valid'] > 0, decoded_example['state/current/speed'], tf.zeros_like(decoded_example['state/current/speed'])),\n",
        "      tf.where(decoded_example['state/current/valid'] > 0, decoded_example['state/current/bbox_yaw'], tf.zeros_like(decoded_example['state/current/bbox_yaw'])),\n",
        "      tf.where(decoded_example['state/current/valid'] > 0, decoded_example['state/current/velocity_x'], tf.zeros_like(decoded_example['state/current/velocity_x'])),\n",
        "      tf.where(decoded_example['state/current/valid'] > 0, decoded_example['state/current/velocity_y'], tf.zeros_like(decoded_example['state/current/velocity_y'])),\n",
        "      tf.where(decoded_example['state/current/valid'] > 0, decoded_example['state/current/vel_yaw'], tf.zeros_like(decoded_example['state/current/vel_yaw']))\n",
        "  ], 1) # shape (128,5,1)\n",
        "\n",
        "  cur_xyz = tf.stack([\n",
        "      decoded_example['state/current/x'], \n",
        "      decoded_example['state/current/y'],\n",
        "      decoded_example['state/current/z'],              \n",
        "  ], 1) # shape (128,3,1)\n",
        "  \n",
        "  #Same as for pasts_states Note, future states are currently not being used and might be able to be removed.\n",
        "  future_states = tf.stack([\n",
        "        # If we use future states speed should probably be included but currently throws an error.\n",
        "        #decoded_example['state/future/speed'],\n",
        "        decoded_example['state/future/bbox_yaw'],\n",
        "        decoded_example['state/future/velocity_x'],\n",
        "        decoded_example['state/future/velocity_y'],\n",
        "        decoded_example['state/future/vel_yaw']\n",
        "    ], 1) # Will have shape (128,5,80)\n",
        "  \n",
        "  #Same as for past_xyz but for future time points\n",
        "  future_xyz = tf.stack([\n",
        "        decoded_example['state/future/x'], \n",
        "        decoded_example['state/future/y'],\n",
        "        decoded_example['state/future/z'],                 \n",
        "  ], 1) # Will have shape (128,3,80)\n",
        "\n",
        "  past_is_valid = decoded_example['state/past/valid'] > 0\n",
        "  current_is_valid = decoded_example['state/current/valid'] > 0\n",
        "  future_is_valid = decoded_example['state/future/valid'] > 0\n",
        "  is_valid = tf.concat([past_is_valid, current_is_valid], axis = 1)\n",
        "\n",
        "  gt_future_is_valid = tf.concat([past_is_valid, current_is_valid, future_is_valid], 1)\n",
        "\n",
        "  # If a sample was not seen at all in the past, we declare the sample as invalid.\n",
        "  sample_is_valid = tf.reduce_any(tf.concat([past_is_valid, current_is_valid], 1), 1)\n",
        "\n",
        "  # Gets the valid object types and set all invalid object types to 0.\n",
        "  object_type = tf.where(sample_is_valid,decoded_example['state/type'], tf.zeros_like(decoded_example['state/type']))\n",
        "\n",
        "  # Get all valid roadgraph types and roadgraph directions and setting all others to 0.\n",
        "  roadgraph_type = tf.where(decoded_example['roadgraph_samples/valid'] > 0,decoded_example['roadgraph_samples/type'], tf.zeros_like(decoded_example['roadgraph_samples/type']))\n",
        "  roadgraph_mask = tf.concat([decoded_example['roadgraph_samples/valid'] > 0, decoded_example['roadgraph_samples/valid'] > 0, decoded_example['roadgraph_samples/valid'] > 0],1)\n",
        "  roadgraph_dir = tf.where(roadgraph_mask, decoded_example['roadgraph_samples/dir'], tf.zeros_like(decoded_example['roadgraph_samples/dir']))\n",
        "    \n",
        "  inputs = {\n",
        "      'past_states': past_states,\n",
        "      'past_xyz': past_xyz,\n",
        "      'past_valid': past_is_valid,\n",
        "\n",
        "      'current_states': cur_states,\n",
        "      'current_xyz': cur_xyz,\n",
        "      'current_valid': decoded_example['state/current/valid'] > 0,\n",
        "      \n",
        "      'future_states': future_states,\n",
        "      'future_xyz': future_xyz,\n",
        "      'future_valid': decoded_example['state/future/valid'] > 0,\n",
        "      \n",
        "      'object_type': decoded_example['state/type'],\n",
        "      'is_valid': is_valid,\n",
        "      'tracks_to_predict': decoded_example['state/tracks_to_predict'] > 0,\n",
        "      'sample_is_valid': sample_is_valid,\n",
        "\n",
        "      'roadgraph_samples/dir' :  roadgraph_dir,\n",
        "      'roadgraph_samples/id' :  decoded_example['roadgraph_samples/id'],\n",
        "      'roadgraph_samples/type' :  roadgraph_type,\n",
        "      'roadgraph_samples/valid' :  decoded_example['roadgraph_samples/valid'],\n",
        "      'roadgraph_samples/xyz' :  decoded_example['roadgraph_samples/xyz'],\n",
        "      \n",
        "      # For finding the gt in training and validation set\n",
        "      'final_x': tf.reshape(decoded_example['state/future/x'][:,-1], (-1,1)), \n",
        "      'final_y': tf.reshape(decoded_example['state/future/y'][:,-1], (-1,1))\n",
        "  }\n",
        "  return inputs  "
      ],
      "metadata": {
        "id": "VzLqcrE-dYIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(path, num_used_files, bs, alt_cizgi = 0):\n",
        "    data_files = os.listdir(path)\n",
        "    data_files = data_files[alt_cizgi: alt_cizgi + num_used_files]          \n",
        "    dataset_plot = tf.data.TFRecordDataset([os.path.join(path, f) for f in data_files], num_parallel_reads=4)\n",
        "    dataset = dataset_plot.map(_parse_model_ready)     \n",
        "    dataset = dataset.batch(bs)                   \n",
        "    return dataset, dataset_plot"
      ],
      "metadata": {
        "id": "L3qOLdMxdRxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Construct the training set on a normalized map**"
      ],
      "metadata": {
        "id": "DNziQXhWdPPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def const_train_set(inputs, gt_func):\n",
        "\n",
        "    #Normalizes the input that needs to be normalized\n",
        "    normalized_inputs = normalize(inputs)\n",
        "\n",
        "    bs, num_agents, num_features, past_time_steps = normalized_inputs['past_states'].shape\n",
        "    time_steps = past_time_steps + 1 \n",
        "    bs, road_num, _ = inputs['roadgraph_samples/id'].shape\n",
        "    \n",
        "    # Adds past and current states to input vector\n",
        "    X_past = tf.reshape(normalized_inputs['past_states'], [bs, num_agents * num_features, past_time_steps])\n",
        "    X_current = tf.reshape(normalized_inputs['current_states'], [bs, num_agents * num_features, 1])\n",
        "    X_time = tf.concat([X_past, X_current], axis = 2)\n",
        "\n",
        "    # Adds object types to input vector\n",
        "    A = tf.broadcast_to(tf.expand_dims(inputs['object_type'], axis = 2), shape=[bs, num_agents, time_steps])\n",
        "    X_time = tf.concat([X_time, A], axis=1)\n",
        "\n",
        "    A = tf.broadcast_to(tf.expand_dims(tf.cast(inputs['tracks_to_predict'], dtype = 'float32'), axis = 2), shape=[bs, num_agents, time_steps])\n",
        "    X_time = tf.concat([X_time, A], axis=1)\n",
        "\n",
        "    # Adds roadgraph types (e.g. LaneCenter-Freeway, Roadline-SolidSingleYellow)\n",
        "    X_spatial = tf.cast(inputs['roadgraph_samples/type'], dtype = 'float32')\n",
        "\n",
        "    # Adds the direction of the roadgraph\n",
        "    A = tf.cast(tf.reshape(inputs['roadgraph_samples/dir'][:,:,0], (bs, 20000,1)), dtype = 'float32')\n",
        "    X_spatial = tf.concat([X_spatial, A], axis=1)\n",
        "    A = tf.cast(tf.reshape(inputs['roadgraph_samples/dir'][:,:,1], (bs, 20000,1)), dtype = 'float32')\n",
        "    X_spatial = tf.concat([X_spatial, A], axis=1)\n",
        "    A = tf.cast(tf.reshape(inputs['roadgraph_samples/dir'][:,:,2], (bs, 20000,1)), dtype = 'float32')\n",
        "    X_spatial = tf.concat([X_spatial, A], axis=1)\n",
        "\n",
        "    Y = gt_func(inputs, normalized_inputs)\n",
        "\n",
        "    return X_time, X_spatial, Y, normalized_inputs['center_x'], normalized_inputs['center_y']"
      ],
      "metadata": {
        "id": "dKSG1Yozdlws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(inputs):\n",
        "  # This function moves all scense to origin\n",
        "  \n",
        "  # Finds the center values for each scene in the batch. shape of center_? = (128,)\n",
        "  center_x, center_y, center_z = find_center(inputs['roadgraph_samples/xyz'], inputs['roadgraph_samples/valid'])\n",
        "  \n",
        "  # Normalizes the roadgraph by subtracting the center value.\n",
        "  # The transposes are there to make the broadcasting work properly in the correct dimension.\n",
        "  # Also extracts the valid values and sets all other values to zero.\n",
        "  bs = inputs['roadgraph_samples/xyz'].shape[0]\n",
        "  roadgraph_x_norm = tf.transpose(tf.transpose(inputs['roadgraph_samples/xyz'][:,:,0]) - center_x)\n",
        "  roadgraph_x_norm_valid = tf.where(tf.reshape(inputs['roadgraph_samples/valid']>0, (bs,-1)), roadgraph_x_norm, tf.zeros_like(roadgraph_x_norm))\n",
        "\n",
        "  roadgraph_y_norm = tf.transpose(tf.transpose(inputs['roadgraph_samples/xyz'][:,:,1]) - center_x)\n",
        "  roadgraph_y_norm_valid = tf.where(tf.reshape(inputs['roadgraph_samples/valid']>0, (bs,-1)), roadgraph_y_norm, tf.zeros_like(roadgraph_y_norm))\n",
        "  \n",
        "  roadgraph_z_norm = tf.transpose(tf.transpose(inputs['roadgraph_samples/xyz'][:,:,2]) - center_x)\n",
        "  roadgraph_z_norm_valid = tf.where(tf.reshape(inputs['roadgraph_samples/valid']>0, (bs,-1)), roadgraph_z_norm, tf.zeros_like(roadgraph_z_norm))\n",
        "  \n",
        "\n",
        "  # Normalizes the past coordinates by subtracting the center value.\n",
        "  # The transposes are there to make the broadcasting work properly in the correct dimension.\n",
        "  # Also extracts the valid values and sets all other values to zero.\n",
        "  bs, num_agents, _, past_timesteps = inputs['past_xyz'].shape\n",
        "  past_states_x_norm = inputs['past_xyz'][:,:,0,:] - tf.transpose(tf.broadcast_to(tf.broadcast_to(center_x,(num_agents,bs)), (past_timesteps,num_agents,bs)), (2,1,0))\n",
        "  past_states_x_norm_valid = tf.where(inputs['past_valid'], past_states_x_norm, tf.zeros_like(past_states_x_norm))\n",
        "  past_states_y_norm = inputs['past_xyz'][:,:,1,:] - tf.transpose(tf.broadcast_to(tf.broadcast_to(center_y,(num_agents,bs)), (past_timesteps,num_agents,bs)), (2,1,0))\n",
        "  past_states_y_norm_valid = tf.where(inputs['past_valid'], past_states_y_norm, tf.zeros_like(past_states_y_norm))\n",
        "  past_states_z_norm = inputs['past_xyz'][:,:,2,:] - tf.transpose(tf.broadcast_to(tf.broadcast_to(center_z,(num_agents,bs)), (past_timesteps,num_agents,bs)), (2,1,0))\n",
        "  past_states_z_norm_valid = tf.where(inputs['past_valid'], past_states_z_norm, tf.zeros_like(past_states_z_norm))\n",
        "\n",
        "  past_xyz = tf.stack([past_states_x_norm_valid, past_states_y_norm_valid, past_states_z_norm_valid], 2)\n",
        "  past_states =  tf.concat([past_xyz, inputs['past_states']], 2)\n",
        "\n",
        "\n",
        "  # Normalizes the current coordinates by subtracting the center value.\n",
        "  # The transposes are there to make the broadcasting work properly in the correct dimension.\n",
        "  # Also extracts the valid values and sets all other values to zero.\n",
        "  bs, num_agents, _, current_timesteps = inputs['current_xyz'].shape\n",
        "  curr_states_x_norm = inputs['current_xyz'][:,:,0,:] - tf.transpose(tf.broadcast_to(tf.broadcast_to(center_x,(num_agents,bs)), (current_timesteps,num_agents,bs)), (2,1,0))\n",
        "  curr_states_x_norm_valid = tf.where(inputs['current_valid'], curr_states_x_norm, tf.zeros_like(curr_states_x_norm))\n",
        "  curr_states_y_norm = inputs['current_xyz'][:,:,1,:] - tf.transpose(tf.broadcast_to(tf.broadcast_to(center_y,(num_agents,bs)), (current_timesteps,num_agents,bs)), (2,1,0))\n",
        "  curr_states_y_norm_valid = tf.where(inputs['current_valid'], curr_states_y_norm, tf.zeros_like(curr_states_y_norm))\n",
        "  curr_states_z_norm = inputs['current_xyz'][:,:,2,:] - tf.transpose(tf.broadcast_to(tf.broadcast_to(center_z,(num_agents,bs)), (current_timesteps,num_agents,bs)), (2,1,0))\n",
        "  curr_states_z_norm_valid = tf.where(inputs['current_valid'], curr_states_z_norm, tf.zeros_like(curr_states_z_norm))  \n",
        "  \n",
        "  curr_xyz = tf.stack([curr_states_x_norm_valid, curr_states_y_norm_valid, curr_states_z_norm_valid], 2)\n",
        "  curr_states =  tf.concat([curr_xyz, inputs['current_states']], 2)\n",
        "\n",
        "  # Normalizes the future coordinates by subtracting the center value.\n",
        "  # The transposes are there to make the broadcasting work properly in the correct dimension.\n",
        "  # Also extracts the valid values and sets all other values to zero.\n",
        "  bs, num_agents, _, future_timesteps = inputs['future_xyz'].shape\n",
        "  future_states_x_norm = inputs['future_xyz'][:,:,0,:] - tf.transpose(tf.broadcast_to(tf.broadcast_to(center_x,(num_agents,bs)), (future_timesteps,num_agents,bs)), (2,1,0))\n",
        "  future_states_x_norm_valid = tf.where(inputs['future_valid'], future_states_x_norm, tf.zeros_like(future_states_x_norm))\n",
        "  future_states_y_norm = inputs['future_xyz'][:,:,1,:] - tf.transpose(tf.broadcast_to(tf.broadcast_to(center_y,(num_agents,bs)), (future_timesteps,num_agents,bs)), (2,1,0))\n",
        "  future_states_y_norm_valid = tf.where(inputs['future_valid'], future_states_y_norm, tf.zeros_like(future_states_y_norm))\n",
        "  future_states_z_norm = inputs['future_xyz'][:,:,2,:] - tf.transpose(tf.broadcast_to(tf.broadcast_to(center_z,(num_agents,bs)), (future_timesteps,num_agents,bs)), (2,1,0))\n",
        "  future_states_z_norm_valid = tf.where(inputs['future_valid'], future_states_z_norm, tf.zeros_like(future_states_z_norm))  \n",
        "  \n",
        "  future_xyz = tf.stack([future_states_x_norm_valid, future_states_y_norm_valid, future_states_z_norm_valid], 2)\n",
        "  future_states =  tf.concat([future_xyz, inputs['future_states']], 2)\n",
        "\n",
        "\n",
        "  normalized_inputs = {\n",
        "      'past_states': past_states,\n",
        "      'current_states': curr_states,\n",
        "      'future_states': future_states,\n",
        "      'roadgraph_samples/x':  tf.reshape(roadgraph_x_norm_valid, (bs,-1,1)),\n",
        "      'roadgraph_samples/y':  tf.reshape(roadgraph_y_norm_valid, (bs,-1,1)),\n",
        "      'roadgraph_samples/z':  tf.reshape(roadgraph_z_norm_valid, (bs,-1,1)),\n",
        "\n",
        "      # For finding the gt in training and validation set\n",
        "      'final_x': tf.reshape(future_states_x_norm_valid[:,:,-1], (bs, num_agents, 1)),\n",
        "      'final_y': tf.reshape(future_states_y_norm_valid[:,:,-1], (bs, num_agents, 1)),\n",
        "      'center_x': center_x,\n",
        "      'center_y': center_y\n",
        "  }\n",
        "  return normalized_inputs "
      ],
      "metadata": {
        "id": "16pvnW6qdnOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_center(roadgraph_xyz, roadgraph_valid):\n",
        "    #finds the center of the map by finding the largest and the smallest value of the roads, and average these values.\n",
        "    \n",
        "    #Finds the max for each scene in the batch by taking the max over the second dimension (the road values).\n",
        "    #Invalid data is filled with large negative value in order to not be the maximum value and thus influence the result.\n",
        "    #The same approach is used for finding the minimum value but with a large positive value.\n",
        "    max_x = tf.reduce_max(tf.where(roadgraph_valid[:,:,0]>0, roadgraph_xyz[:,:,0], tf.ones_like(roadgraph_xyz[:,:,0])*-10000000), 1)\n",
        "    min_x = tf.reduce_min(tf.where(roadgraph_valid[:,:,0]>0, roadgraph_xyz[:,:,0], tf.ones_like(roadgraph_xyz[:,:,0])*10000000), 1)\n",
        "    \n",
        "    max_y = tf.reduce_max(tf.where(roadgraph_valid[:,:,0]>0, roadgraph_xyz[:,:,1], tf.ones_like(roadgraph_xyz[:,:,1])*-10000000), 1)\n",
        "    min_y = tf.reduce_min(tf.where(roadgraph_valid[:,:,0]>0, roadgraph_xyz[:,:,1], tf.ones_like(roadgraph_xyz[:,:,1])*10000000), 1)\n",
        "    \n",
        "    max_z = tf.reduce_max(tf.where(roadgraph_valid[:,:,0]>0, roadgraph_xyz[:,:,2], tf.ones_like(roadgraph_xyz[:,:,2])*-10000000), 1)\n",
        "    min_z = tf.reduce_min(tf.where(roadgraph_valid[:,:,0]>0, roadgraph_xyz[:,:,2], tf.ones_like(roadgraph_xyz[:,:,2])*10000000), 1)\n",
        "\n",
        "    center_x = (max_x + min_x)/2\n",
        "    center_y = (max_y + min_y)/2\n",
        "    center_z = (max_z + min_z)/2\n",
        "\n",
        "    return center_x, center_y, center_z"
      ],
      "metadata": {
        "id": "qmsdYQR4dsqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Goal Prediction Part**"
      ],
      "metadata": {
        "id": "CRzYuzzidlNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Construct the LSTM model for end point prediction**"
      ],
      "metadata": {
        "id": "Fale7FQ_ddtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GoalPredModel(num_states_steps, latent_dim, num_agents_to_predict, num_end_points):\n",
        "      \"\"\"Goal Prediction model\n",
        "      param num_states_steps: 10 past + 1 current scenario\n",
        "      param lated_dim: the dimension of the latent diemsions between the LSTM blocks\n",
        "      param num_agents_to_predict: Number of agents that the system needs to predict\n",
        "      param num_end_points: Number of end points the system needs to predict = 2 (x and y coordinate for 1 point)\n",
        "      \"\"\"\n",
        "\n",
        "      #The length of the large input vector. Depends on how many features we include.\n",
        "      input_vec_length = 1280 \n",
        "      num_static_features = 20000*4\n",
        "      time_inputs = Input(shape=(input_vec_length, num_states_steps))\n",
        "      static_inputs = Input(shape=(num_static_features,1))\n",
        "      a0 = Input(shape=(latent_dim,), name='a0')\n",
        "      c0 = Input(shape=(latent_dim,), name='c0')\n",
        "      a = a0\n",
        "      c = c0\n",
        "\n",
        "      # Create the encoder\n",
        "      for t in range(num_states_steps):\n",
        "          # Select the \"t\"th time step vector from enc_input. \n",
        "          x_time = Lambda(lambda z: z[:, :, t])(time_inputs)\n",
        "          # Reshape x \n",
        "          x_time = Reshape((1, input_vec_length))(x_time)\n",
        "          x_time = BatchNormalization()(x_time)                                 \n",
        "          # LSTM_cell\n",
        "          a, _, c = LSTM(latent_dim, kernel_initializer=GlorotUniform(), return_state = True)(x_time, initial_state=[a, c])\n",
        "\n",
        "      #Convolve spatial features\n",
        "      x_spatial = Conv1D(4, 5, strides = 2, padding = 'same', kernel_initializer=GlorotUniform())(static_inputs)\n",
        "      x_spatial = MaxPool1D(pool_size=2, strides=2)(x_spatial)\n",
        "      x_spatial = Conv1D(8, 5, strides = 2, padding = 'same', kernel_initializer=GlorotUniform())(x_spatial)\n",
        "      x_spatial = MaxPool1D(pool_size=2, strides=2)(x_spatial)\n",
        "      x_spatial = Conv1D(8, 5, strides = 2, padding = 'same', kernel_initializer=GlorotUniform())(x_spatial)\n",
        "      x_spatial = MaxPool1D(pool_size=2, strides=2)(x_spatial)\n",
        "      x_spatial = Conv1D(16, 5, strides = 2, padding = 'same', kernel_initializer=GlorotUniform())(x_spatial)\n",
        "      x_spatial = MaxPool1D(pool_size=2, strides=2)(x_spatial)\n",
        "\n",
        "      x_spatial = tf.keras.layers.Flatten()(x_spatial)\n",
        "      x_spatial = Dropout(rate=DROPOUT_RATE_SPATIAL)(x_spatial)\n",
        "      a = tf.keras.layers.Flatten()(a)\n",
        "\n",
        "      x = tf.concat([a, x_spatial], axis = 1)\n",
        "\n",
        "      # To hold the outputs\n",
        "      outputs = []\n",
        "      out = Dense(num_agents_to_predict*num_end_points, kernel_initializer=GlorotUniform())(x)\n",
        "      outputs.append(out)\n",
        "\n",
        "      model = Model(inputs=[time_inputs, static_inputs, a0, c0], outputs=outputs)\n",
        "      return model"
      ],
      "metadata": {
        "id": "L_82RgoNd8Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get the ground truth data**"
      ],
      "metadata": {
        "id": "nVR2qc-aeap_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gt_final(inputs, normalized_inputs):\n",
        "    \"\"\"Returns a concatenated tensor of shape (bs,256,1), where 256=128*2 (for x and y coordinate)\n",
        "    The only values that are non-zero in the return tensor are the ones that we actually are interested in.\n",
        "    Observe the first 8 of the indicies represent the x and the last 8 represents the y coordinate\n",
        "    \"\"\"\n",
        "    \n",
        "    Yx = normalized_inputs['final_x']\n",
        "    Yy = normalized_inputs['final_y']\n",
        "    tracks = inputs['tracks_to_predict']\n",
        "\n",
        "    Yx = tf.math.multiply(Yx, tf.expand_dims(tf.cast(tracks, dtype = 'float32'), axis = 2))\n",
        "    Yx = Yx[:,0:8,:]\n",
        "\n",
        "    Yy = tf.math.multiply(Yy, tf.expand_dims(tf.cast(tracks, dtype = 'float32'), axis = 2))\n",
        "    Yy = Yy[:,0:8,:]\n",
        "    \n",
        "    Y = tf.concat([Yx, Yy], axis = 1)\n",
        "    return Y"
      ],
      "metadata": {
        "id": "m_bfBf4WeZ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualization**"
      ],
      "metadata": {
        "id": "8XehI1eTesfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions in this subsection is from:\n",
        "\n",
        "Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp,\n",
        "Charles R. Qi, Yin Zhou, Zoey Yang, Aur’elien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander\n",
        "McCauley, Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motion forecasting for autonomous\n",
        "driving: The waymo open motion dataset.\n",
        "6"
      ],
      "metadata": {
        "id": "zj8ywslAfifU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_figure_and_axes(size_pixels):\n",
        "  \"\"\"Initializes a unique figure and axes for plotting.\"\"\"\n",
        "  fig, ax = plt.subplots(1, 1, num=uuid.uuid4())\n",
        "\n",
        "  # Sets output image to pixel resolution.\n",
        "  dpi = 100\n",
        "  size_inches = size_pixels / dpi\n",
        "  fig.set_size_inches([size_inches, size_inches])\n",
        "  fig.set_dpi(dpi)\n",
        "  fig.set_facecolor('white')\n",
        "  ax.set_facecolor('white')\n",
        "  ax.xaxis.label.set_color('black')\n",
        "  ax.tick_params(axis='x', colors='black')\n",
        "  ax.yaxis.label.set_color('black')\n",
        "  ax.tick_params(axis='y', colors='black')\n",
        "  fig.set_tight_layout(True)\n",
        "  ax.grid(False)\n",
        "  return fig, ax"
      ],
      "metadata": {
        "id": "NrS04AbhesIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fig_canvas_image(fig):\n",
        "  \"\"\"Returns a [H, W, 3] uint8 np.array image from fig.canvas.tostring_rgb().\"\"\"\n",
        "  # Just enough margin in the figure to display xticks and yticks.\n",
        "  fig.subplots_adjust(\n",
        "      left=0.08, bottom=0.08, right=0.98, top=0.98, wspace=0.0, hspace=0.0)\n",
        "  fig.canvas.draw()\n",
        "  data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  return data.reshape(fig.canvas.get_width_height()[::-1] + (3,))"
      ],
      "metadata": {
        "id": "mC47k-3MeY37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_colormap(num_agents):\n",
        "  \"\"\"Compute a color map array of shape [num_agents, 4].\"\"\"\n",
        "  colors = cm.get_cmap('jet', num_agents)\n",
        "  colors = colors(range(num_agents))\n",
        "  np.random.shuffle(colors)\n",
        "  colors[0] = [0.,0.,0.5,1.,]\n",
        "  colors[1] = [0.5,0.,0.,1.,]\n",
        "  colors[2] = [0.,0.5,0.,1.,]\n",
        "\n",
        "  colors[3] = [0.5,0.5,0.,1.,]\n",
        "  colors[4] = [0.5,0.,0.5,1.,]\n",
        "  colors[5] = [0.,0.5,0.5,1.,]\n",
        "\n",
        "  colors[6] = [0.2,0.7,0.1,1.,]\n",
        "  colors[7] = [0.5,0.5,0.5,1.,]\n",
        "  return colors"
      ],
      "metadata": {
        "id": "ATNViRJ3e44J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_viewport(all_states, all_states_mask):\n",
        "  \"\"\"Gets the region containing the data.\n",
        "\n",
        "  Args:\n",
        "    all_states: states of agents as an array of shape [num_agents, num_steps,\n",
        "      2].\n",
        "    all_states_mask: binary mask of shape [num_agents, num_steps] for\n",
        "      `all_states`.\n",
        "\n",
        "  Returns:\n",
        "    center_y: float. y coordinate for center of data.\n",
        "    center_x: float. x coordinate for center of data.\n",
        "    width: float. Width of data.\n",
        "  \"\"\"\n",
        "  valid_states = all_states[all_states_mask]\n",
        "  all_y = valid_states[..., 1]\n",
        "  all_x = valid_states[..., 0]\n",
        "\n",
        "  center_y = (np.max(all_y) + np.min(all_y)) / 2\n",
        "  center_x = (np.max(all_x) + np.min(all_x)) / 2\n",
        "\n",
        "  range_y = np.ptp(all_y)\n",
        "  range_x = np.ptp(all_x)\n",
        "\n",
        "  width = max(range_y, range_x)\n",
        "\n",
        "  return center_y, center_x, width"
      ],
      "metadata": {
        "id": "9DcOOVffe7Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_one_step(states,\n",
        "                       mask,\n",
        "                       roadgraph,\n",
        "                       title,\n",
        "                       center_y,\n",
        "                       center_x,\n",
        "                       width,\n",
        "                       color_map,\n",
        "                       size_pixels=1000):\n",
        "  \"\"\"Generate visualization for a single step.\"\"\"\n",
        "\n",
        "  # Create figure and axes.\n",
        "  fig, ax = create_figure_and_axes(size_pixels=size_pixels)\n",
        "\n",
        "  # Plot roadgraph.\n",
        "  rg_pts = roadgraph[:, :2].T\n",
        "  ax.plot(rg_pts[0, :], rg_pts[1, :], 'k.', alpha=1, ms=2)\n",
        "\n",
        "  masked_x = states[:, 0][mask]\n",
        "  masked_y = states[:, 1][mask]\n",
        "  colors = color_map[mask]\n",
        "\n",
        "  # Plot agent current position.\n",
        "  ax.scatter(\n",
        "      masked_x,\n",
        "      masked_y,\n",
        "      marker='o',\n",
        "      linewidths=3,\n",
        "      color=colors,\n",
        "  )\n",
        "\n",
        "  # Title.\n",
        "  ax.set_title(title)\n",
        "\n",
        "  # Set axes.  Should be at least 10m on a side and cover 160% of agents.\n",
        "  size = max(10, width * 1.0)\n",
        "  ax.axis([\n",
        "      -size / 2 + center_x, size / 2 + center_x, -size / 2 + center_y,\n",
        "      size / 2 + center_y\n",
        "  ])\n",
        "  ax.set_aspect('equal')\n",
        "\n",
        "  image = fig_canvas_image(fig)\n",
        "  plt.close(fig)\n",
        "  return image"
      ],
      "metadata": {
        "id": "sh784FwCe9El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_all_agents_smooth(decoded_example,\n",
        "    size_pixels=1000,\n",
        "):\n",
        "  \"\"\"Visualizes all agent predicted trajectories in a serie of images.\n",
        "\n",
        "  Args:\n",
        "    decoded_example: Dictionary containing agent info about all modeled agents.\n",
        "    size_pixels: The size in pixels of the output image.\n",
        "\n",
        "  Returns:\n",
        "    T of [H, W, 3] uint8 np.arrays of the drawn matplotlib's figure canvas.\n",
        "  \"\"\"\n",
        "  # [num_agents, num_past_steps, 2] float32.\n",
        "  past_states = tf.stack(\n",
        "      [decoded_example['state/past/x'], decoded_example['state/past/y']],\n",
        "      -1).numpy()\n",
        "\n",
        "  num_agents, num_past_steps, _ = past_states.shape\n",
        "\n",
        "  past_states_mask = decoded_example['state/past/valid'].numpy() > 0.0\n",
        "\n",
        "  # [num_agents, 1, 2] float32.\n",
        "  current_states = tf.stack(\n",
        "      [decoded_example['state/current/x'], decoded_example['state/current/y']],\n",
        "      -1).numpy()\n",
        "  current_states_mask = decoded_example['state/current/valid'].numpy() > 0.0\n",
        "\n",
        "  # [num_agents, num_future_steps, 2] float32.\n",
        "  future_states = tf.stack(\n",
        "      [decoded_example['state/future/x'], decoded_example['state/future/y']],\n",
        "      -1).numpy()\n",
        "  num_future_steps = future_states.shape[1]\n",
        "  future_states_mask = decoded_example['state/future/valid'].numpy() > 0.0\n",
        "  \n",
        "  # [num_points, 3] float32.\n",
        "  roadgraph_xyz = decoded_example['roadgraph_samples/xyz'].numpy()\n",
        "\n",
        "  color_map = get_colormap(num_agents)\n",
        "\n",
        "  # [num_agens, num_past_steps + 1 + num_future_steps, depth] float32.\n",
        "  all_states = np.concatenate([past_states, current_states, future_states], 1)\n",
        "\n",
        "  # [num_agens, num_past_steps + 1 + num_future_steps] float32.\n",
        "  all_states_mask = np.concatenate(\n",
        "      [past_states_mask, current_states_mask, future_states_mask], 1)\n",
        "  \n",
        "  center_y, center_x, width = get_viewport(all_states, all_states_mask)\n",
        "\n",
        "  images = []\n",
        "\n",
        "  # Generate images from past time steps.\n",
        "  for i, (s, m) in enumerate(\n",
        "      zip(\n",
        "          np.split(past_states, num_past_steps, 1),\n",
        "          np.split(past_states_mask, num_past_steps, 1))):\n",
        "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
        "                            'past: %d' % (num_past_steps - i), center_y,\n",
        "                            center_x, width, color_map, size_pixels)\n",
        "    images.append(im)\n",
        "\n",
        "  # Generate one image for the current time step.\n",
        "  s = current_states\n",
        "  m = current_states_mask\n",
        "\n",
        "  im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz, 'current', center_y,\n",
        "                          center_x, width, color_map, size_pixels)\n",
        "  images.append(im)\n",
        "\n",
        "  predict_agent_mask = decoded_example['state/tracks_to_predict'].numpy() > 0.0\n",
        "  predict_agent_mask = tf.broadcast_to(tf.expand_dims(predict_agent_mask, axis=1), [num_agents, num_future_steps])\n",
        "  future_states_mask = np.logical_and(future_states_mask, predict_agent_mask)\n",
        "  print(future_states_mask[0:10,-1])\n",
        "  \n",
        "  # Generate images from future time steps.\n",
        "  for i, (s, m) in enumerate(\n",
        "      zip(\n",
        "          np.split(future_states, num_future_steps, 1),\n",
        "          np.split(future_states_mask, num_future_steps, 1))):\n",
        "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
        "                            'future: %d' % (i + 1), center_y, center_x, width,\n",
        "                            color_map, size_pixels)\n",
        "    images.append(im)\n",
        "\n",
        "  return images"
      ],
      "metadata": {
        "id": "7rZFx9QgfAKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_all_agents_smooth_w_np(decoded_example, prediction_file_x, prediction_file_y,\n",
        "    size_pixels=1000,):\n",
        "  \"\"\"Visualizes all agent predicted trajectories in a serie of images.\n",
        "\n",
        "  Args:\n",
        "    decoded_example: Dictionary containing agent info about all modeled agents.\n",
        "    size_pixels: The size in pixels of the output image.\n",
        "\n",
        "  Returns:\n",
        "    T of [H, W, 3] uint8 np.arrays of the drawn matplotlib's figure canvas.\n",
        "  \"\"\"\n",
        "  # [num_agents, num_past_steps, 2] float32.\n",
        "  past_states = tf.stack(\n",
        "      [decoded_example['state/past/x'], decoded_example['state/past/y']],\n",
        "      -1).numpy()\n",
        "\n",
        "  num_agents, num_past_steps, _ = past_states.shape\n",
        "\n",
        "  past_states_mask = decoded_example['state/past/valid'].numpy() > 0.0\n",
        "\n",
        "  # [num_agents, 1, 2] float32.\n",
        "  current_states = tf.stack(\n",
        "      [decoded_example['state/current/x'], decoded_example['state/current/y']],\n",
        "      -1).numpy()\n",
        "  current_states_mask = decoded_example['state/current/valid'].numpy() > 0.0\n",
        "\n",
        "  # [num_agents, num_future_steps, 2] float32.\n",
        "  future_states = np.stack(\n",
        "      [prediction_file_x, prediction_file_y],\n",
        "      -1)\n",
        "  num_future_steps = future_states.shape[1]\n",
        "  future_states_mask = decoded_example['state/future/valid'].numpy() > 0.0\n",
        "  \n",
        "  # [num_points, 3] float32.\n",
        "  roadgraph_xyz = decoded_example['roadgraph_samples/xyz'].numpy()\n",
        "\n",
        "  color_map = get_colormap(num_agents)\n",
        "\n",
        "  # [num_agens, num_past_steps + 1 + num_future_steps, depth] float32.\n",
        "  all_states = np.concatenate([past_states, current_states, future_states], 1)\n",
        "\n",
        "  # [num_agens, num_past_steps + 1 + num_future_steps] float32.\n",
        "  all_states_mask = np.concatenate(\n",
        "      [past_states_mask, current_states_mask, future_states_mask], 1)\n",
        "  \n",
        "  center_y, center_x, width = get_viewport(all_states, all_states_mask)\n",
        "\n",
        "  images = []\n",
        "\n",
        "  # Generate images from past time steps.\n",
        "  for i, (s, m) in enumerate(\n",
        "      zip(\n",
        "          np.split(past_states, num_past_steps, 1),\n",
        "          np.split(past_states_mask, num_past_steps, 1))):\n",
        "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
        "                            'past: %d' % (num_past_steps - i), center_y,\n",
        "                            center_x, width, color_map, size_pixels)\n",
        "    images.append(im)\n",
        "\n",
        "  # Generate one image for the current time step.\n",
        "  s = current_states\n",
        "  m = current_states_mask\n",
        "\n",
        "  im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz, 'current', center_y,\n",
        "                          center_x, width, color_map, size_pixels)\n",
        "  images.append(im)\n",
        "\n",
        "  predict_agent_mask = decoded_example['state/tracks_to_predict'].numpy() > 0.0\n",
        "  predict_agent_mask = tf.broadcast_to(tf.expand_dims(predict_agent_mask, axis=1), [num_agents, num_future_steps])\n",
        "  future_states_mask = np.logical_and(future_states_mask, predict_agent_mask)\n",
        "  #print(future_states_mask[0:10,-1])\n",
        "  \n",
        "  # Generate images from future time steps.\n",
        "  for i, (s, m) in enumerate(\n",
        "      zip(\n",
        "          np.split(future_states, num_future_steps, 1),\n",
        "          np.split(future_states_mask, num_future_steps, 1))):\n",
        "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
        "                            'future: %d' % (i + 1), center_y, center_x, width,\n",
        "                            color_map, size_pixels)\n",
        "    images.append(im)\n",
        "\n",
        "  return images"
      ],
      "metadata": {
        "id": "0u28XLdufJkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot**"
      ],
      "metadata": {
        "id": "vicE8wP0fz9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_plots(visualization_file, predicted_values_x, predicted_values_y, gt_values_x, gt_values_y):\n",
        "    # Plot the end point for the agents we are interested in. First for the GT case (not the predicted one).\n",
        "    #images = visualize_all_agents_smooth(visualization_file)\n",
        "    #plt.rcParams['figure.figsize'] = [10, 10]\n",
        "    #plt.imshow(images[-1])\n",
        "    #plt.show()\n",
        "\n",
        "    # Plot the end point for the agents we are interested in. Second for the predicted one.\n",
        "    gt_file_x = visualization_file['state/future/x'].numpy()\n",
        "    gt_file_x[:,-1][0:8] = gt_values_x \n",
        "    gt_file_y = visualization_file['state/future/y'].numpy()\n",
        "    gt_file_y[:,-1][0:8] = gt_values_y\n",
        "\n",
        "    images = visualize_all_agents_smooth_w_np(visualization_file, gt_file_x, gt_file_y)\n",
        "    plt.rcParams['figure.figsize'] = [10, 10]\n",
        "    plt.imshow(images[-1])\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the end point for the agents we are interested in. Second for the predicted one.\n",
        "    prediction_file_x = visualization_file['state/future/x'].numpy()\n",
        "    prediction_file_x[:,-1][0:8] = predicted_values_x \n",
        "    prediction_file_y = visualization_file['state/future/y'].numpy()\n",
        "    prediction_file_y[:,-1][0:8] = predicted_values_y\n",
        "\n",
        "    images = visualize_all_agents_smooth_w_np(visualization_file, prediction_file_x, prediction_file_y)\n",
        "    plt.rcParams['figure.figsize'] = [10, 10]\n",
        "    plt.imshow(images[-1])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VXDgR1sifMpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helper functions for training the goal prediction model**"
      ],
      "metadata": {
        "id": "LbMLWgrjf9I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_1_epoch(epoch, train_dataset):\n",
        "    # Set the batch losses to be an empty list to only contain losses from this epochs batches.\n",
        "    logits_visualize, center_x_visualize, center_y_visualize, y_gt_visualize = 0, 0, 0, 0\n",
        "    train_batch_losses = []\n",
        "    \n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(train_dataset):\n",
        "        # Send in 1 batch to the function train_step batch.\n",
        "        # 'batch' is a DICTIONARY containing the past position, velocity, current position,\n",
        "        # velocity etc. for all the objects in the scenario for all the time indices for\n",
        "        # all batches. \n",
        "        train_batch_time, train_batch_static, y_batch_train, center_x, center_y = const_train_set(batch, get_gt_final)\n",
        "        bs = train_batch_time.shape[0]\n",
        "        a0 = np.zeros((bs, LATENT_DIM))\n",
        "        c0 = np.zeros((bs, LATENT_DIM))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Run the forward pass\n",
        "            logits = model([train_batch_time, train_batch_static ,a0,c0], training=True)\n",
        "            y_batch_train = tf.reshape(y_batch_train, (y_batch_train.shape[0:2]))\n",
        "\n",
        "            # Saves gt and predicted values to be able to visualize the results \n",
        "            if step == STEP_TO_VISUALIZE and epoch % EPOCH_TO_VISUALIZE == 0:# and epoch != 0:\n",
        "              y_gt_visualize = y_batch_train\n",
        "              logits_visualize = logits\n",
        "              center_x_visualize = center_x\n",
        "              center_y_visualize = center_y\n",
        "            \n",
        "            # Compute the loss value\n",
        "            weights = batch['tracks_to_predict'][:,:8]\n",
        "            weights = tf.concat([weights, weights], axis = 1)\n",
        "\n",
        "            valid_logits = tf.boolean_mask(logits, weights)\n",
        "            valid_gt = tf.boolean_mask(y_batch_train, weights)\n",
        "            loss_value = loss_fn(valid_gt, valid_logits)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        train_batch_losses.append(loss_value)\n",
        "    # Prints the training loss after each epoch\n",
        "    print(\"The mean training loss after %d epochs: %.4f\"% (epoch, float(sum(train_batch_losses)/len(train_batch_losses))))\n",
        "    train_epoch_losses.append(float(sum(train_batch_losses)/len(train_batch_losses)))\n",
        "    return logits_visualize, center_x_visualize, center_y_visualize, y_gt_visualize"
      ],
      "metadata": {
        "id": "o1vNtnd1gHR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_val_loss(valid_dataset): \n",
        "  valid_batch_losses = [] \n",
        "  logits_visualize_valid, center_x_valid_visualize, center_y_valid_visualize, y_gt_visualize_valid = 0,0,0,0\n",
        "  for step, batch in enumerate(valid_dataset):\n",
        "    valid_time_batch, valid_static_batch,y_batch_valid, center_x_valid, center_y_valid = const_train_set(batch, get_gt_final)\n",
        "    bs = valid_time_batch.shape[0]\n",
        "    a0 = np.zeros((bs, LATENT_DIM))\n",
        "    c0 = np.zeros((bs, LATENT_DIM))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Run the forward pass of the layer.\n",
        "      logits = model([valid_time_batch,valid_static_batch,a0,c0], training=False)\n",
        "      y_batch_valid = tf.reshape(y_batch_valid, (y_batch_valid.shape[0:2]))\n",
        "\n",
        "      # Saves gt and predicted values to be able to visualize the results \n",
        "      if step == STEP_TO_VISUALIZE and epoch % EPOCH_TO_VISUALIZE == 0 and epoch != 0:\n",
        "        y_gt_visualize_valid = y_batch_valid\n",
        "        logits_visualize_valid = logits\n",
        "        center_x_valid_visualize = center_x_valid\n",
        "        center_y_valid_visualize = center_y_valid\n",
        "      \n",
        "      # Compute the loss value for this minibatch.\n",
        "      loss_value = loss_fn(y_batch_valid, logits)\n",
        "\n",
        "    valid_batch_losses.append(loss_value)\n",
        "\n",
        "  print(\"The mean validation loss after %d epochs: %.4f\"% (epoch, float(sum(valid_batch_losses)/len(valid_batch_losses))))\n",
        "  valid_epoch_losses.append(float(sum(valid_batch_losses)/len(valid_batch_losses)))\n",
        "\n",
        "  return logits_visualize_valid, center_x_valid_visualize, center_y_valid_visualize, y_gt_visualize_valid"
      ],
      "metadata": {
        "id": "yeLQEAbngg5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoalPredModel(11, LATENT_DIM, 8, 2)\n",
        "\n",
        "lr = START_LR\n",
        "optimizer = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "train_epoch_losses = []\n",
        "valid_epoch_losses = []"
      ],
      "metadata": {
        "id": "2BlVRmpagl84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_for_visualization = tf.data.TFRecordDataset(FILENAME, compression_type='')    # parse the tf record\n",
        "dataset_for_visualization_valid = tf.data.TFRecordDataset(VALIDATION_FILENAME, compression_type='')    # parse the tf record\n",
        "\n",
        "\n",
        "for step, batch in enumerate(dataset_for_visualization.batch(BS)):\n",
        "    if step == STEP_TO_VISUALIZE:\n",
        "        for example in batch:\n",
        "            visualization_file = tf.io.parse_single_example(example, features_description)\n",
        "            break\n",
        "\n",
        "for step, batch in enumerate(dataset_for_visualization_valid.batch(BSV)):\n",
        "    if step == STEP_TO_VISUALIZE:\n",
        "        for example in batch:\n",
        "            visualization_file_valid = tf.io.parse_single_example(example, features_description)\n",
        "            break\n",
        "\n",
        "visualization_file_for_prediction = dataset_for_visualization.map(_parse_model_ready)\n",
        "visualization_file_for_prediction_valid = dataset_for_visualization_valid.map(_parse_model_ready)"
      ],
      "metadata": {
        "id": "sMzUmWyAhDdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the train sets\n",
        "train_path = '/content/waymo_dataset/uncompressed/tf_example/training/'\n",
        "valid_path = '/content/waymo_dataset/uncompressed/tf_example/validation/'\n",
        "train_dataset, train_dataset_plot = get_dataset(train_path, NUM_USED_FILES, BS)\n",
        "valid_dataset, _ = get_dataset(valid_path, NUM_USED_FILES_VALID, BSV)\n",
        "test_dataset, _ = get_dataset(valid_path, NUM_USED_FILES_VALID, BSV, alt_cizgi = 75)"
      ],
      "metadata": {
        "id": "X9F6kPpXhOP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train**"
      ],
      "metadata": {
        "id": "EBhIAdHyiNvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print('\\nStart of epoch %d' % (epoch,))\n",
        "    \n",
        "    # Run 1 epoch on the training dataset and update model parameters\n",
        "    logits_visualize, center_x_visualize, center_y_visualize, y_gt_visualize = train_1_epoch(epoch, train_dataset)\n",
        "\n",
        "    # Calculate and print the validation loss\n",
        "    logits_visualize_valid, center_x_valid_visualize, center_y_valid_visualize, y_gt_visualize_valid = calc_val_loss(valid_dataset)\n",
        "        \n",
        "    # Visualize the results\n",
        "    if epoch % EPOCH_TO_VISUALIZE == 0 and epoch != 0:\n",
        "        calculate_plots(visualization_file, logits_visualize[0][0:8]+center_x_visualize[0], logits_visualize[0][8:]+center_y_visualize[0], y_gt_visualize[0][0:8]+center_x_visualize[0], y_gt_visualize[0][8:]+center_y_visualize[0])  #adding the center values to the logits to make the vizualisation in the original coordinate system\n",
        "        calculate_plots(visualization_file_valid, logits_visualize_valid[0][0:8]+center_x_valid_visualize[0], logits_visualize_valid[0][8:]+center_y_valid_visualize[0], y_gt_visualize_valid[0][0:8]+center_x_valid_visualize[0], y_gt_visualize_valid[0][8:]+center_y_valid_visualize[0])  #adding the center values to the logits to make the vizualisation in the original coordinate system"
      ],
      "metadata": {
        "id": "ZG_ZOZeciJKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the results\n",
        "plt.plot(np.arange(0, len(train_epoch_losses)), train_epoch_losses)\n",
        "plt.plot(np.arange(0, len(valid_epoch_losses)), valid_epoch_losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zgr3uGqQiuts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trajectory Prediction Part**"
      ],
      "metadata": {
        "id": "eAzZAuYzi1Bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculate the GT for future steps**"
      ],
      "metadata": {
        "id": "XSAz7Qini91K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trajectory_gt(inputs, normalized_inputs):\n",
        "    \"\"\"Returns a concatenated tensor of shape (bs,1280,1), where 1280=8*2*80 (for x and y coordinate)\n",
        "    The only values that are non-zero in the return tensor are the ones that we actually are interested in.\n",
        "    \"\"\"\n",
        "    \n",
        "    Yx = normalized_inputs['future_states'][:,:,0,:]   # is of shape (bs, #agents, time steps in future (80)) -- x coordinate\n",
        "    Yy = normalized_inputs['future_states'][:,:,1,:]   # is of shape (bs, #agents, time steps in future (80)) -- y coordinate\n",
        "    tracks = inputs['tracks_to_predict']\n",
        "\n",
        "    Yx = tf.math.multiply(Yx, tf.expand_dims(tf.cast(tracks, dtype = 'float32'), axis = 2))\n",
        "    Yx = Yx[:,0:8,:]\n",
        "\n",
        "    Yy = tf.math.multiply(Yy, tf.expand_dims(tf.cast(tracks, dtype = 'float32'), axis = 2))\n",
        "    Yy = Yx[:,0:8,:]\n",
        "    \n",
        "    Y = tf.concat([Yx, Yy], axis = 2)\n",
        "\n",
        "    Y_final = tf.expand_dims(Y[:,0,:], axis=0)\n",
        "    for i in range(7):\n",
        "        Y_final = tf.concat([Y_final, tf.expand_dims(Y[:,i,:], axis=0)], axis = 0)\n",
        "\n",
        "    return Y_final"
      ],
      "metadata": {
        "id": "P_7qF52dixjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Construct the LSTM model for the trajectory prediction part**"
      ],
      "metadata": {
        "id": "qLBzUTYWjFGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_trajPred(num_states_steps, latent_dim):\n",
        "    #The length of the large input vector. Depends on how many features we include.\n",
        "    input_vec_length = 1280 \n",
        "    num_static_features = 20000*4\n",
        "    time_inputs = Input(shape=(input_vec_length, num_states_steps))\n",
        "    final_points = Input(shape = (16,1))\n",
        "    static_inputs = Input(shape=(num_static_features,1))\n",
        "    a0 = Input(shape=(latent_dim,))\n",
        "    c0 = Input(shape=(latent_dim,))\n",
        "    a = a0\n",
        "    c = c0\n",
        "\n",
        "    # Create the encoder\n",
        "    for t in range(num_states_steps):\n",
        "        # Select the \"t\"th time step vector from enc_input. \n",
        "        x_time = Lambda(lambda z: z[:, :, t])(time_inputs)\n",
        "        # Reshape x \n",
        "        x_time = Reshape((1, input_vec_length))(x_time)\n",
        "        reshaped_fp = Reshape((1,16))(final_points)\n",
        "\n",
        "        x_time = tf.concat([reshaped_fp, x_time], axis = 2)\n",
        "        x_time = BatchNormalization()(x_time)                                 \n",
        "\n",
        "        # LSTM_cell\n",
        "        a, _, c = LSTM(latent_dim, kernel_initializer=GlorotUniform(), return_state = True)(x_time, initial_state=[a, c])\n",
        "\n",
        "    #Convolve spatial features\n",
        "    x_spatial = Conv1D(4, 5, strides = 2, padding = 'same', kernel_initializer=GlorotUniform())(static_inputs)\n",
        "    x_spatial = MaxPool1D(pool_size=2, strides=2)(x_spatial)\n",
        "    x_spatial = Conv1D(8, 5, strides = 2, padding = 'same', kernel_initializer=GlorotUniform())(x_spatial)\n",
        "    x_spatial = MaxPool1D(pool_size=2, strides=2)(x_spatial)\n",
        "    x_spatial = Conv1D(8, 5, strides = 2, padding = 'same', kernel_initializer=GlorotUniform())(x_spatial)\n",
        "    x_spatial = MaxPool1D(pool_size=2, strides=2)(x_spatial)\n",
        "    x_spatial = Conv1D(16, 5, strides = 2, padding = 'same', kernel_initializer=GlorotUniform())(x_spatial)\n",
        "    x_spatial = MaxPool1D(pool_size=2, strides=2)(x_spatial)\n",
        "\n",
        "    x_spatial = tf.keras.layers.Flatten()(x_spatial)\n",
        "    x_spatial = Dropout(rate=DROPOUT_RATE_SPATIAL)(x_spatial)\n",
        "    \n",
        "    a = tf.keras.layers.Flatten()(a)\n",
        "    x = tf.concat([a, x_spatial], axis = 1)\n",
        "    output_encoder = Dense(LATENT_DIM, kernel_initializer=GlorotUniform())(x)\n",
        "\n",
        "    return time_inputs, static_inputs, final_points, a0, c0, output_encoder"
      ],
      "metadata": {
        "id": "qCKq79vljDwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_trajPred(output_encoder, outputs, num_future_steps, latent_dim):\n",
        "    a0_dec = output_encoder\n",
        "    c0_dec = Input(shape=(latent_dim,))\n",
        "    a = a0_dec\n",
        "    c = c0_dec\n",
        "    zero_input = Input(shape=(128, 2, 2))\n",
        "\n",
        "    for t in range(int(num_future_steps/10)):\n",
        "        zi_t = Lambda(lambda z: z[:, t, :, :])(zero_input)\n",
        "        zi_t = Reshape((1, 2*2))(zi_t)\n",
        "        a, _, c = LSTM(latent_dim, kernel_initializer=GlorotUniform(), return_state = True)(zi_t, initial_state=[a, c])\n",
        "        out_t = Dense(160, kernel_initializer=GlorotUniform())(a)\n",
        "        outputs.append(out_t)\n",
        "  \n",
        "    return c0_dec, zero_input"
      ],
      "metadata": {
        "id": "ImIXuHVrjQMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TrajPredModel(num_states_steps, latent_dim, num_future_steps):\n",
        "      \n",
        "      time_inputs, static_inputs, final_points, a0, c0, output_encoder = encoder_trajPred(num_states_steps, latent_dim)\n",
        "\n",
        "      outputs = []\n",
        "      c0_dec, zero_input = decoder_trajPred(output_encoder, outputs, num_future_steps, latent_dim)\n",
        "\n",
        "      model = Model(inputs=[time_inputs, static_inputs, final_points, a0, c0, c0_dec, zero_input], outputs=outputs)\n",
        "      return model"
      ],
      "metadata": {
        "id": "AbqiNK6ZjSIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helper functions for training**"
      ],
      "metadata": {
        "id": "mbjrLK34jUmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_1_epoch_trajPred(epoch, train_dataset):\n",
        "  logits_visualize, center_x_visualize, center_y_visualize, y_gt_visualize = 0, 0, 0, 0\n",
        "  \n",
        "  # Set the batch losses to be an empty list to only contain losses from this epochs batches.\n",
        "  train_batch_losses = []\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, batch in enumerate(train_dataset):\n",
        "\n",
        "      # Send in 1 batch to the function train_step batch.\n",
        "      # 'batch' is a DICTIONARY containing the past position, velocity, current position,\n",
        "      # velocity etc. etc for all the objects in the scenario for all the time indices for\n",
        "      # all batches. \n",
        "      train_batch_time, train_batch_static, y_batch_train, center_x, center_y = const_train_set(batch, get_trajectory_gt)\n",
        "      bs = train_batch_time.shape[0]\n",
        "      a0 = np.zeros((bs, LATENT_DIM))\n",
        "      c0 = np.zeros((bs, LATENT_DIM))\n",
        "      c0_dec = np.zeros((bs, LATENT_DIM))\n",
        "      zero_input = np.zeros((bs, 128, 2, 2))\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          # Run the forward pass of the layer.\n",
        "          final_points = get_gt_final(batch, normalize(batch))\n",
        "          logits = traj_model([train_batch_time, train_batch_static, final_points, a0, c0, c0_dec, zero_input], training=True)\n",
        "          \n",
        "          # Compute the loss value for this minibatch.\n",
        "          loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "      # Use the gradient tape to automatically retrieve\n",
        "      # the gradients of the trainable variables with respect to the loss.\n",
        "      grads = tape.gradient(loss_value, traj_model.trainable_weights)\n",
        "\n",
        "      # Run one step of gradient descent by updating\n",
        "      # the value of the variables to minimize the loss\n",
        "      optimizer.apply_gradients(zip(grads, traj_model.trainable_weights))\n",
        "\n",
        "      train_batch_losses.append(loss_value)\n",
        "\n",
        "  # Prints the training loss after each epoch\n",
        "  if len(train_batch_losses) != 0:\n",
        "      print(\"The mean training loss after %d epochs: %.4f\"% (epoch, float(sum(train_batch_losses)/len(train_batch_losses))))\n",
        "      traj_train_epoch_losses.append(float(sum(train_batch_losses)/len(train_batch_losses)))"
      ],
      "metadata": {
        "id": "7tvHTpTqjTxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_validation_loss(valid_dataset):\n",
        "    logits_visualize_valid, center_x_valid_visualize, center_y_valid_visualize, y_gt_visualize_valid = 0,0,0,0\n",
        "    valid_batch_losses = []\n",
        "\n",
        "    # Calculates and prints the validation loss\n",
        "    for step, batch in enumerate(valid_dataset):\n",
        "      valid_time_batch, valid_static_batch, y_batch_valid, center_x_valid, center_y_valid = const_train_set(batch, get_trajectory_gt)\n",
        "      bs = valid_time_batch.shape[0]\n",
        "      a0 = np.zeros((bs, LATENT_DIM))\n",
        "      c0 = np.zeros((bs, LATENT_DIM))\n",
        "      c0_dec = np.zeros((bs, LATENT_DIM))\n",
        "      zero_input = np.zeros((bs, 128, 2, 2))\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        # Run the forward pass of the layer.\n",
        "        final_points = get_gt_final(batch, normalize(batch))\n",
        "        logits = traj_model([valid_time_batch, valid_static_batch, final_points, a0, c0, c0_dec, zero_input], training=False)\n",
        "        \n",
        "        # Compute the loss value for this minibatch.\n",
        "        loss_value = loss_fn(y_batch_valid, logits)\n",
        "\n",
        "      valid_batch_losses.append(loss_value)\n",
        "\n",
        "    if len(valid_batch_losses) != 0:\n",
        "        print(\"The mean validation loss after %d epochs: %.4f\"% (epoch, float(sum(valid_batch_losses)/len(valid_batch_losses))))\n",
        "        traj_valid_epoch_losses.append(float(sum(valid_batch_losses)/len(valid_batch_losses)))"
      ],
      "metadata": {
        "id": "C1Ow4eCLkeGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train the model on the Trajectory prediction task**"
      ],
      "metadata": {
        "id": "wo0_EBSHk5Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = EPOCHS\n",
        "\n",
        "traj_train_epoch_losses = []\n",
        "traj_valid_epoch_losses = []\n",
        "\n",
        "traj_model = TrajPredModel(11, LATENT_DIM, 80)"
      ],
      "metadata": {
        "id": "1qvGXT63k2bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = START_LR_TRAJ\n",
        "optimizer = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()"
      ],
      "metadata": {
        "id": "CykRfuAclA34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    print('\\nStart of epoch %d' % (epoch,))\n",
        "    \n",
        "    train_1_epoch_trajPred(epoch, train_dataset)\n",
        "    calculate_validation_loss(valid_dataset)"
      ],
      "metadata": {
        "id": "6q59fjVflCNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(0, len(traj_train_epoch_losses)), traj_train_epoch_losses)\n",
        "plt.plot(np.arange(0, len(traj_valid_epoch_losses)), traj_valid_epoch_losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Knd1BWQhlPNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the system**"
      ],
      "metadata": {
        "id": "kgPfJsmNspjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the test dataset\n",
        "test_dataset"
      ],
      "metadata": {
        "id": "gIpAw9SostPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(test_dataset): \n",
        "  test_batch_ade = [] \n",
        "  test_batch_fde = []\n",
        "  for step, batch in enumerate(test_dataset):\n",
        "    test_time_batch, test_static_batch, y_batch_test, center_x_test, center_y_test = const_train_set(batch, get_trajectory_gt)\n",
        "    _, _, y_batch_test_end_point, _, _ = const_train_set(batch, get_gt_final)\n",
        "    bs = test_time_batch.shape[0]\n",
        "    a0 = np.zeros((bs, LATENT_DIM))\n",
        "    c0 = np.zeros((bs, LATENT_DIM))\n",
        "    c0_dec = np.zeros((bs, LATENT_DIM))\n",
        "    zero_input = np.zeros((bs, 128, 2, 2))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted_final_points = model([test_time_batch, test_static_batch, a0, c0], training=False)\n",
        "      predicted_trajectories = traj_model([test_time_batch, test_static_batch, predicted_final_points, a0, c0, c0_dec, zero_input], training=False)\n",
        "            \n",
        "    # Compute the loss value for this minibatch.\n",
        "    ade = np.sqrt(loss_fn(y_batch_test, predicted_trajectories))\n",
        "    test_batch_ade.append(ade)\n",
        "\n",
        "    fde = np.sqrt(loss_fn(y_batch_test_end_point, predicted_final_points))\n",
        "    test_batch_fde.append(fde)\n",
        "  return test_batch_ade, test_batch_fde"
      ],
      "metadata": {
        "id": "o5wNSlUYsugY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch_ade, test_batch_fde = testing(test_dataset)\n",
        "print('Mean ade:', sum(test_batch_ade)/(len(test_batch_ade)))\n",
        "print('Mean fde:',  sum(test_batch_fde)/(len(test_batch_fde)))"
      ],
      "metadata": {
        "id": "niQvos5eswmh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}